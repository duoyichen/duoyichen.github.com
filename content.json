{"meta":{"title":"Duoyi's Blog","subtitle":"一个学习的小天地","description":"<p>技术改变世界，学习改变人生</p><p>欢迎交流与分享经验!</p>","author":"DuoyiChen","url":"http://www.duoyichen.xyz"},"pages":[{"title":"about","date":"2018-10-01T18:42:22.000Z","updated":"2018-10-02T11:22:02.516Z","comments":true,"path":"about/index.html","permalink":"http://www.duoyichen.xyz/about/index.html","excerpt":"","text":"积土成山，风雨兴焉；积水成渊，蛟龙生焉。不积跬步，无以至千里；不积小流，无以成江海！"},{"title":"categories","date":"2018-10-01T18:39:28.000Z","updated":"2018-10-01T19:14:11.376Z","comments":true,"path":"categories/index.html","permalink":"http://www.duoyichen.xyz/categories/index.html","excerpt":"","text":""},{"title":"links","date":"2018-10-01T19:46:09.000Z","updated":"2018-10-02T10:39:51.429Z","comments":true,"path":"links/index.html","permalink":"http://www.duoyichen.xyz/links/index.html","excerpt":"","text":""},{"title":"repository","date":"2018-10-01T19:46:50.000Z","updated":"2018-10-01T19:49:19.569Z","comments":true,"path":"repository/index.html","permalink":"http://www.duoyichen.xyz/repository/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-10-01T19:02:54.000Z","updated":"2018-10-01T19:13:49.922Z","comments":true,"path":"tags/index.html","permalink":"http://www.duoyichen.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"OpenStack Mitaka + CentOS7.2 安装部署 06","slug":"openstack-m-centos-6","date":"2018-10-01T20:36:18.000Z","updated":"2018-10-01T23:21:34.155Z","comments":true,"path":"2018-10/openstack-m-centos-6/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/openstack-m-centos-6/","excerpt":"","text":"八、安装存储服务（cinder）8.1安装controller节点8.1.1创建数据库12345mysql -uroot -popenstack -e&quot;CREATE DATABASE cinder;&quot;mysql -uroot -popenstack -e&quot;GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;controller&apos; \\ IDENTIFIED BY &apos;openstack&apos;;&quot;mysql -uroot -popenstack -e&quot;GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; \\ IDENTIFIED BY &apos;openstack&apos;;&quot; 8.1.2创建cinder用户1openstack user create --domain default --password=cinder cinder 8.1.3添加admin角色到cinder用户1openstack role add --project service --user cinder admin 8.1.4创建cinder服务12openstack service create --name cinder --description \"OpenStack Block Storage\" volumeopenstack service create --name cinderv2 --description \"OpenStack Block Storage\" volumev2 8.1.5创建cinder的api123456789101112openstack endpoint create --region RegionOne \\ volume public http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\ volume internal http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\ volume admin http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\ volumev2 public http://controller:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\ volumev2 internal http://controller:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\ volumev2 admin http://controller:8776/v2/%\\(tenant_id\\)s 8.1.6安装cinder1yum install -y openstack-cinder 8.1.7配置数据库1openstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:openstack@controller/cinder 8.1.8配置rabbitmq1234openstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_host controlleropenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_password openstack 8.1.9配置keystone12345678910openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name serviceopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinderopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken password cinder 8.1.10锁路径1openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp 8.1.11初始化数据库1su -s /bin/sh -c \"cinder-manage db sync\" cinder 8.1.12配置nova使用cinder1penstack-config --set /etc/nova/nova.conf cinder os_region_name RegionOne 8.1.13重启nova-api1systemctl restart openstack-nova-api.service 8.1.14启动cinder12systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.servicesystemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 8.2安装存储节点8.2.1安装lvm1yum install lvm2 -y 8.2.2启动lvm12systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service 8.2.3配置物理卷并创建卷组12pvcreate /dev/sdbvgcreate cinder-volumes /dev/sdb 8.2.4配置lvm(/etc/lvm/lvm.conf)123devices &#123;...filter = [ &quot;a/sdb/&quot;, &quot;r/.*/&quot;] 8.2.4安装cinder1yum install openstack-cinder targetcli python-keystone -y 8.2.5配置rabbitmq1234openstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_host controlleropenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_password openstack 8.2.6配置keystone12345678910openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name serviceopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinderopenstack-config --set /etc//cinder/cinder.conf keystone_authtoken password cinder 8.2.7配置ip和锁路径12openstack-config --set /etc//cinder/cinder.conf DEFAULT my_ip 10.0.20.30openstack-config --set /etc//cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp 8.2.8配置cinder使用lvm12345openstack-config --set /etc/cinder/cinder.conf lvm volume_driver cinder.volume.drivers.lvm.LVMVolumeDriveropenstack-config --set /etc/cinder/cinder.conf lvm volume_group cinder-volumesopenstack-config --set /etc/cinder/cinder.conf lvm iscsi_protocol iscsiopenstack-config --set /etc/cinder/cinder.conf lvm iscsi_helper lioadmopenstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends lvm 8.2.9配置数据库1openstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:openstack@controller/cinder 8.2.10启动cinder12systemctl enable openstack-cinder-volume.service target.servicesystemctl start openstack-cinder-volume.service target.service","categories":[{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/categories/云计算/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"openstack","slug":"openstack","permalink":"http://www.duoyichen.xyz/tags/openstack/"},{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/tags/云计算/"}]},{"title":"OpenStack Mitaka + CentOS7.2 安装部署 05","slug":"openstack-m-centos-5","date":"2018-10-01T20:36:17.000Z","updated":"2018-10-01T23:21:15.572Z","comments":true,"path":"2018-10/openstack-m-centos-5/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/openstack-m-centos-5/","excerpt":"","text":"七、安装dashboard7.1安装1yum install openstack-dashboard -y 7.2配置123456789101112131415161718sed -i &quot;s#OPENSTACK_HOST = \\&quot;127.0.0.1\\&quot;#OPENSTACK_HOST = \\&quot;controller\\&quot;#g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s#ALLOWED_HOSTS = \\[&apos;horizon.example.com&apos;, &apos;localhost&apos;\\]#ALLOWED_HOSTS = \\[&apos;*&apos;, \\]#g&quot; /etc/openstack-dashboard/local_settingsecho &quot;SESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;&quot; &gt;&gt; /etc/openstack-dashboard/local_settingssed -i &quot;s@#CACHES = &#123;@CACHES = &#123;@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# &apos;default&apos;: &#123;@ &apos;default&apos;: &#123;@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;,@ &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;,@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# &apos;LOCATION&apos;: &apos;127.0.0.1:11211&apos;,@ &apos;LOCATION&apos;: &apos;controller:11211&apos;,@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# &#125;,@ &#125;,@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@#OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = False@OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@OPENSTACK_KEYSTONE_DEFAULT_ROLE = \\&quot;_member_\\&quot;@OPENSTACK_KEYSTONE_DEFAULT_ROLE = \\&quot;user\\&quot;@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@#OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;@OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@#OPENSTACK_API_VERSIONS@OPENSTACK_API_VERSIONS@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# \\&quot;identity\\&quot;: 3,@ \\&quot;identity\\&quot;: 3,@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# \\&quot;volume\\&quot;: 2@ \\&quot;volume\\&quot;: 2@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;s@# \\&quot;compute\\&quot;: 2,@ \\&quot;compute\\&quot;: 2,@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;133s@#@@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;132s@#@@g&quot; /etc/openstack-dashboard/local_settingssed -i &quot;60s@#@@g&quot; /etc/openstack-dashboard/local_settings 7.3重启apache1systemctl restart httpd","categories":[{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/categories/云计算/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"openstack","slug":"openstack","permalink":"http://www.duoyichen.xyz/tags/openstack/"},{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/tags/云计算/"}]},{"title":"OpenStack Mitaka + CentOS7.2 安装部署 04","slug":"openstack-m-centos-4","date":"2018-10-01T20:36:16.000Z","updated":"2018-10-01T23:19:50.890Z","comments":true,"path":"2018-10/openstack-m-centos-4/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/openstack-m-centos-4/","excerpt":"","text":"六、安装网络服务（neutron）6.1安装控制节点（controller）6.1.1创建数据库123mysql -uroot -popenstack -e\"CREATE DATABASE neutron;\"mysql -uroot -popenstack -e\"GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'controller' IDENTIFIED BY 'openstack';\"mysql -uroot -popenstack -e\"GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'openstack';\" 6.1.2创建neutron用户1openstack user create --domain default --password=neutron neutron 6.1.3添加admin角色到neutron角色1openstack role add --project service --user neutron admin 6.1.4创建neutron服务1openstack service create --name neutron --description \"OpenStack Networking\" network 6.1.5创建neutron的api123openstack endpoint create --region RegionOne network public http://controller:9696openstack endpoint create --region RegionOne network internal http://controller:9696openstack endpoint create --region RegionOne network admin http://controller:9696 6.1.6安装相关组件1yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables 6.1.7配置数据库连接1openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:openstack@controller/neutron 6.1.8配置使用ml2插件123openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins routeropenstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True 6.1.9配置rabbitmq1234openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_host controlleropenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstack 6.1.10配置keystone12345678910openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name serviceopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutronopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutron 6.1.11配置nova12345678910openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes Trueopenstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf nova auth_type passwordopenstack-config --set /etc/neutron/neutron.conf nova project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf nova user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf nova region_name RegionOneopenstack-config --set /etc/neutron/neutron.conf nova project_name serviceopenstack-config --set /etc/neutron/neutron.conf nova username novaopenstack-config --set /etc/neutron/neutron.conf nova password nova 6.1.12配置锁路径1openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp 6.1.13配置ml21234567openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan,vxlanopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers linuxbridge,l2populationopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_securityopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks provideropenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 1:1000openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True 6.1.14配置Linuxbridge代理123456openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth2openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan Trueopenstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan local_ip 10.0.20.10openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan l2_population Trueopenstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 6.1.15配置DHCP代理123openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriveropenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasqopenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True 6.1.16配置layer－3代理12openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriveropenstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge 6.1.17配置nova1234567891011openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf neutron auth_type passwordopenstack-config --set /etc/nova/nova.conf neutron project_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron user_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron region_name RegionOneopenstack-config --set /etc/nova/nova.conf neutron project_name serviceopenstack-config --set /etc/nova/nova.conf neutron username neutronopenstack-config --set /etc/nova/nova.conf neutron password neutronopenstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy Trueopenstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret METADATA_SECRET 6.1.18配置元数据代理12openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip controlleropenstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret METADATA_SECRET 6.1.19创建软连接1ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 6.1.20同步数据库12su -s /bin/sh -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf \\ --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\" neutron 6.1.21重启nova-api1systemctl restart openstack-nova-api.service 6.1.22启动neutron12345678systemctl enable neutron-server.service \\ neutron-linuxbridge-agent.service neutron-dhcp-agent.service \\ neutron-metadata-agent.servicesystemctl start neutron-server.service \\ neutron-linuxbridge-agent.service neutron-dhcp-agent.service \\ neutron-metadata-agent.servicesystemctl enable neutron-l3-agent.servicesystemctl start neutron-l3-agent.service 6.2安装计算节点（compute）6.2.1安装组件1yum install -y openstack-neutron-linuxbridge ebtables ipset 6.2.2配置rabbitmq1234openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_host controlleropenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstack 6.2.3配置keystone12345678910openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name serviceopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutronopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutron 6.2.4配置锁路径1openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp 6.2.5配置Linuxbridge代理123456openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth2openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan Trueopenstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan local_ip 10.0.20.20openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan l2_population Trueopenstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 6.2.6配置nova123456789openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf neutron auth_type passwordopenstack-config --set /etc/nova/nova.conf neutron project_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron user_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron region_name RegionOneopenstack-config --set /etc/nova/nova.conf neutron project_name serviceopenstack-config --set /etc/nova/nova.conf neutron username neutronopenstack-config --set /etc/nova/nova.conf neutron password neutron 6.2.7重启nova-api1systemctl restart openstack-nova-compute.service 6.2.8启动Linuxbridge代理12systemctl enable neutron-linuxbridge-agent.servicesystemctl start neutron-linuxbridge-agent.service 6.2.9 验证安装12345678910root@controller ~ 20:04:49 # neutron agent-list+--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+| id | agent_type | host | availability_zone | alive | admin_state_up | binary |+--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+| 14259fcd-448f-4fb6-ba30-fb5885ebe878 | Metadata agent | controller | | :-) | True | neutron-metadata-agent || 1994d93b-f6cf-4dd3-8a2a-3623b473c675 | L3 agent | controller | nova | :-) | True | neutron-l3-agent || 9c44da60-3e2a-466e-9185-9c30840ab699 | DHCP agent | controller | nova | :-) | True | neutron-dhcp-agent || afd1d41d-d83d-46ec-b916-e5f9e6d3725e | Linux bridge agent | compute | | :-) | True | neutron-linuxbridge-agent || e5006fdb-b45d-4d9c-b129-6fefbad8a3d5 | Linux bridge agent | controller | | :-) | True | neutron-linuxbridge-agent |+--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ 6.2.10创建网络1234567neutron net-create --shared --provider:physical_network provider \\ --provider:network_type flat providerneutron subnet-create --name provider \\ --allocation-pool start=10.0.30.100,end=10.0.30.200 \\ --dns-nameserver 8.8.4.4 --gateway 10.0.30.1 \\ provider 10.0.30.0/24neutron net-update provider --router:external","categories":[{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/categories/云计算/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"openstack","slug":"openstack","permalink":"http://www.duoyichen.xyz/tags/openstack/"},{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/tags/云计算/"}]},{"title":"OpenStack Mitaka + CentOS7.2 安装部署 03","slug":"openstack-m-centos-3","date":"2018-10-01T20:36:15.000Z","updated":"2018-10-01T23:22:09.133Z","comments":true,"path":"2018-10/openstack-m-centos-3/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/openstack-m-centos-3/","excerpt":"","text":"四、镜像服务（glance，在controller）4.1创建数据库12345mysql -uroot -popenstack -e \"CREATE DATABASE glance;\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'controller' \\ IDENTIFIED BY 'openstack';\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\ IDENTIFIED BY 'openstack';\" 4.2获取admin权限1source ~/admin.rc 4.3创建服务和api4.3.1创建glance用户1openstack user create --domain default --password=glance glance 4.3.2添加admin角色到glance用户和service项目上1openstack role add --project service --user glance admin 4.3.3创建glance服务1openstack service create --name glance --description \"OpenStack Image\" image 4.3.4创建galnce的api123openstack endpoint create --region RegionOne image public http://controller:9292openstack endpoint create --region RegionOne image internal http://controller:9292openstack endpoint create --region RegionOne image admin http://controller:9292 4.4安装配置glance4.4.1安装glance1yum install -y openstack-glance 4.4.2配置glance-api的数据库连接12openstack-config --set /etc/glance/glance-api.conf database \\connection mysql+pymysql://glance:openstack@controller/glance 4.4.3配置glance-api的认证12345678910openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name serviceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glanceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken password glanceopenstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone 4.4.4配置镜像存储位置123openstack-config --set /etc/glance/glance-api.conf glance_store stores file,httpopenstack-config --set /etc/glance/glance-api.conf glance_store default_store fileopenstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/ 4.4.5配置glance-registry的数据库连接1openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:openstack@controller/glance 4.4.6配置glance-registry的认证12345678910openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name serviceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glanceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password glanceopenstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone 4.5同步数据库1su -s /bin/sh -c \"glance-manage db_sync\" glance 4.6启动glance1234systemctl enable openstack-glance-api.service \\ openstack-glance-registry.servicesystemctl start openstack-glance-api.service \\ openstack-glance-registry.service 4.7验证操作1234567891011openstack image create \"cirros\" \\ --file cirros-0.3.4-x86_64-disk.img \\ --disk-format qcow2 --container-format bare \\ --public #查看上传镜像 root@controller ~ 08:20:16 # openstack image list+--------------------------------------+--------+--------+| ID | Name | Status |+--------------------------------------+--------+--------+| 293a2b7f-7338-4900-90b7-2c1d6df56021 | cirros | active |+--------------------------------------+--------+--------+ 五、安装compute（计算服务）5.1安装配置控制节点（controller）5.1.1创建数据库123456mysql -uroot -popenstack -e \"CREATE DATABASE nova_api;\"mysql -uroot -popenstack -e \"CREATE DATABASE nova;\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'controller' IDENTIFIED BY 'openstack';\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'openstack';\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'controller' IDENTIFIED BY 'openstack';\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'openstack';\" 5.1.2创建nova用户1openstack user create --domain default --password=nova nova 5.1.3给nova用户添加admn角色1openstack role add --project service --user nova admin 5.1.4创建nova服务1openstack service create --name nova --description \"OpenStack Compute\" compute 5.1.5创建nova的api123openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s 5.1.6安装nova123yum install -y openstack-nova-api openstack-nova-conductor \\ openstack-nova-console openstack-nova-novncproxy \\ openstack-nova-scheduler 5.1.7配置nova的default1openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata 5.1.8配合nova数据库连接1234openstack-config --set /etc/nova/nova.conf \\api_database connection mysql+pymysql://nova:openstack@controller/nova_apiopenstack-config --set /etc/nova/nova.conf \\database connection mysql+pymysql://nova:openstack@controller/nova 5.1.9配置rabbitmq1234openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controlleropenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password openstack 5.1.10配置keystone认证12345678910openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_name serviceopenstack-config --set /etc/nova/nova.conf keystone_authtoken username novaopenstack-config --set /etc/nova/nova.conf keystone_authtoken password nova 5.1.11配置ip和网络123openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.10.10openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron Trueopenstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver 5.1.12配置vnc和glance123openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 10.0.10.10openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 10.0.10.10openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 5.1.13配置锁路径1openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp 5.1.14同步数据库12su -s /bin/sh -c \"nova-manage api_db sync\" novasu -s /bin/sh -c \"nova-manage db sync\" nova 5.1.15启动controller上的nova123456systemctl enable openstack-nova-api.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service 5.1.16验证123456789root@controller ~ 10:20:52 # openstack compute service list+----+------------------+------------+----------+---------+-------+----------------------------+| Id | Binary | Host | Zone | Status | State | Updated At |+----+------------------+------------+----------+---------+-------+----------------------------+| 1 | nova-conductor | controller | internal | enabled | up | 2017-03-09T02:21:25.000000 || 2 | nova-consoleauth | controller | internal | enabled | up | 2017-03-09T02:21:25.000000 || 3 | nova-scheduler | controller | internal | enabled | up | 2017-03-09T02:21:25.000000 |+----+------------------+------------+----------+---------+-------+----------------------------+root@controller ~ 10:21:31 # 5.2计算节点安装（compute）5.2.1 安装nova-compute1yum install -y openstack-nova-compute 5.2.2配置rabbitmq链接1234openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controlleropenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password openstack 5.2.3配置keystone认证12345678910openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_name serviceopenstack-config --set /etc/nova/nova.conf keystone_authtoken username novaopenstack-config --set /etc/nova/nova.conf keystone_authtoken password nova 5.2.4配置IP和网络123openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.10.20openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron Trueopenstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver 5.2.5配置vnc1234openstack-config --set /etc/nova/nova.conf vnc enabled Trueopenstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 10.0.10.20openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html 5.2.6配置glance1openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 5.2.7配置所路径1openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp 5.2.8配置硬件加速123egrep -c '(vmx|svm)' /proc/cpuinfo#如果返回0则执行下面命令非0不做任何操作openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu 5.2.9启动nova-compute12systemctl enable libvirtd.service openstack-nova-compute.servicesystemctl start libvirtd.service openstack-nova-compute.service 5.2.10验证安装1234567891011#在controller节点执行root@controller ~ 11:10:24 # openstack compute service list +----+------------------+------------+----------+---------+-------+----------------------------+| Id | Binary | Host | Zone | Status | State | Updated At |+----+------------------+------------+----------+---------+-------+----------------------------+| 1 | nova-conductor | controller | internal | enabled | up | 2017-03-09T03:10:46.000000 || 2 | nova-consoleauth | controller | internal | enabled | up | 2017-03-09T03:10:46.000000 || 3 | nova-scheduler | controller | internal | enabled | up | 2017-03-09T03:10:55.000000 || 6 | nova-compute | compute | nova | enabled | up | 2017-03-09T03:10:55.000000 |+----+------------------+------------+----------+---------+-------+----------------------------+root@controller ~ 11:10:55 #","categories":[{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/categories/云计算/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"openstack","slug":"openstack","permalink":"http://www.duoyichen.xyz/tags/openstack/"},{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/tags/云计算/"}]},{"title":"OpenStack Mitaka + CentOS7.2 安装部署 02","slug":"openstack-m-centos-2","date":"2018-10-01T20:36:14.000Z","updated":"2018-10-01T23:22:13.728Z","comments":true,"path":"2018-10/openstack-m-centos-2/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/openstack-m-centos-2/","excerpt":"","text":"三、安装keystone（认证服务，在controller）3.1为keystone创建mysql数据库和表123mysql -uroot -popenstack -e \"CREATE DATABASE keystone;\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'controller' IDENTIFIED BY 'openstack';\"mysql -uroot -popenstack -e \"GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'openstack';\" 3.2生成随机令牌1ADMIN_TOKEN=$(openssl rand -hex 10) 3.3安装keystone和相关组件1yum install -y openstack-keystone httpd mod_wsgi 3.4配置token12openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token $ADMIN_TOKENopenstack-config --set /etc/keystone/keystone.conf token provider fernet 3.5配置mysql链接1openstack-config --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:openstack@controller/keystone 3.6初始化数据库1su -s /bin/sh -c \"keystone-manage db_sync\" keystone 3.7初始化fernet keys1keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone 3.8配置apache3.8.1配置servername1sed -i &quot;s@#ServerName www.example.com:80@ServerName controller@g&quot; /etc/httpd/conf/httpd.conf 3.8.2配置keystone配置文件12345678910111213141516171819202122232425262728293031323334cat&gt;&gt;/etc/httpd/conf.d/wsgi-keystone.conf&lt;&lt;EOFListen 5000Listen 35357&lt;VirtualHost *:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat \"%&#123;cu&#125;t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost *:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat \"%&#123;cu&#125;t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;EOF 3.8.3启动aapche12systemctl enable httpd.servicesystemctl start httpd.service 3.9创建keyston服务和节点3.9.1配置token环境变量123export OS_TOKEN=`echo $ADMIN_TOKEN`export OS_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3 3.9.2创建keystone服务1openstack service create --name keystone --description \"OpenStack Identity\" identity 3.9.3创建api123openstack endpoint create --region RegionOne identity public http://controller:5000/v3openstack endpoint create --region RegionOne identity internal http://controller:5000/v3openstack endpoint create --region RegionOne identity admin http://controller:35357/v3 3.10创建域、用户和角色3.10.1创建默认域1openstack domain create --description \"Default Domain\" default 3.10.2创建admin项目1openstack project create --domain default --description \"Admin Project\" admin 3.10.3创建admin用户1openstack user create --domain default --password=admin admin 3.10.4创建admin角色1openstack role create admin 3.10.5添加admin角色到admin项目和用户1openstack role add --project admin --user admin admin 3.10.6创建service项目1openstack project create --domain default --description \"Service Project\" service 3.10.7创建demo项目1openstack project create --domain default --description \"Demo Project\" demo 3.10.8创建demo用户1openstack user create --domain default --password=demo demo 3.10.9创建user角色1openstack role create user 3.10.10添加user角色到demo 项目和用户1openstack role add --project demo --user demo user 3.11验证3.11.1清空环境变量1unset OS_TOKEN OS_URL 3.11.2验证admin用户请求123openstack --os-auth-url http://controller:35357/v3 \\ --os-project-domain-name default --os-user-domain-name default \\ --os-project-name admin --os-username admin token issue 3.11.3验证demo用户请求123openstack --os-auth-url http://controller:5000/v3 \\ --os-project-domain-name default --os-user-domain-name default \\ --os-project-name demo --os-username demo token issue 3.11.4生成admin用户脚本12345678910cat&gt;&gt;~/admin.rc&lt;&lt;EOFexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=adminexport OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2EOF 3.11.5生成demo用户脚本12345678910cat&gt;&gt;~/demo.rc&lt;&lt;EOFexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=DEMO_PASSexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2EOF","categories":[{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/categories/云计算/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"openstack","slug":"openstack","permalink":"http://www.duoyichen.xyz/tags/openstack/"},{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/tags/云计算/"}]},{"title":"OpenStack Mitaka + CentOS7.2 安装部署 01","slug":"openstack-m-centos","date":"2018-10-01T20:36:13.000Z","updated":"2018-10-01T23:22:18.426Z","comments":true,"path":"2018-10/openstack-m-centos/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/openstack-m-centos/","excerpt":"","text":"一、环境约定和基础包安装1.1环境约定 提示：请先初步理解openstack的各组件功能，阅读此部署文档，文档中不做详细介绍。 主机名 管理IP 私有ip 公有ip 操作系统 controller eth0:10.0.10.10 eth1:10.0.20.10 eth2:10.0.30.10 CenOS7.2-x64 compute eth0:10.0.10.20 eth1:10.0.20.20 eth2:10.0.30.20 CenOS7.2-x64 cinder eth0:10.0.10.30 eth1:10.0.20.30 — CenOS7.2-x64 1.2环境配置（在所有节点执行）12345678910111213141516171819202122systemctl stop firewalld.servicesystemctl disable firewalld.serviceyum install -y wgetmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backwget http://mirrors.aliyun.com/repo/Centos-7.repo -P /etc/yum.repos.d/mv /etc/yum.repos.d/Centos-7.repo /etc/yum.repos.d/CentOS-Base.reporpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpmyum install -y epel-release yum install -y centos-release-openstack-mitakased -i \"s@#baseurl@baseurl@g\" /etc/yum.repos.d/epel.reposed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/epel.reposed -i \"s#http://download.fedoraproject.org/pub#https://mirrors.tuna.tsinghua.edu.cn#g\" /etc/yum.repos.d/epel.reposed -i \"s#http://elrepo.org/linux#https://mirror.tuna.tsinghua.edu.cn/elrepo#g\" /etc/yum.repos.d/elrepo.reposed -i \"s#mirror.centos.org#mirrors.163.com#g\" /etc/yum.repos.d/CentOS-OpenStack-mitaka.repo yum clean allyum makecacheyum install -y tree lrzsz nmap python-pip vim net-tools ntp iperf iftop screen zip unzip gcc gcc-c++ cmakeyum install -y python-openstackclient openstack-selinux ntp openstack-utilssystemctl start ntpdsystemctl enable ntpdsetenforce 0sed -i 's#SELINUX=enforcing#SELINUX=disable#g' /etc/selinux/config 1.3选择执行（可以不执行）123456789101112131415cp /etc/vimrc /etc/vimrc.backcat&gt;&gt;/etc/vimrc&lt;&lt;EOFset nuset tabstop=2set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936set termencoding=utf-8set encoding=utf-8EOF. /etc/vimrcecho \"export LC_ALL=C\" &gt;&gt; /etc/profileecho \"unset MAILCHECK\" &gt;&gt; /etc/profileecho \"export PS1='\\[\\e[31;1m\\]\\u@\\[\\e[34;1m\\]\\h \\[\\e[36;1m\\]\\w \\[\\e[33;1m\\]\\t # \\[\\e[37;1m\\]'\" &gt;&gt; /etc/profileecho \"export GREP_OPTIONS='--color=auto' GREP_COLOR='1;33'\" &gt;&gt; /etc/profileecho \" * - nofile 65536\" &gt;&gt; /etc/security/limits.confsource /etc/profile 二、controller(控制节点)部署2.1安装配置mysql2.1.1安装1yum install -y mariadb mariadb-server MySQL-python 123456789101112131415cat&gt;&gt;/etc/my.cnf&lt;&lt;EOF[client-server][mysqld]symbolic-links=0bind-address = 0.0.0.0default-storage-engine = innodbinnodb_file_per_tablecollation-server = utf8_general_ciinit-connect = 'SET NAMES utf8'character-set-server = utf8max_connections=1000!includedir /etc/my.cnf.dEOFopenstack-config --set /usr/lib/systemd/system/mariadb.service Service LimitNOFILE 10000openstack-config --set /usr/lib/systemd/system/mariadb.service Service LimitNPROC 10000 2.1.2启动mysql12systemctl enable mariadb.servicesystemctl start mariadb.service 2.1.3配置mysql密码1mysqladmin -uroot password \"openstack\" 2.2安装配置mongodb2.2.1安装1yum install -y mongodb-server mongodb 2.2.2配置mongod（/etc/mongod.conf ）12sed -i \"s#bind_ip = 127.0.0.1#bind_ip = 0.0.0.0#g\" /etc/mongod.confsed -i \"s@#smallfiles = true@smallfiles = true@g\" /etc/mongod.conf 2.2.3启动mongodb12systemctl enable mongod.servicesystemctl start mongod.service 2.3安装配置rabbitmq2.3.1安装rabbitmq1yum install -y rabbitmq-server 启动rabbitmq12systemctl enable rabbitmq-server.servicesystemctl start rabbitmq-server.service 2.3.3配置rabbitmq12rabbitmqctl add_user openstack openstackrabbitmqctl set_permissions openstack \".*\" \".*\" \".*\" 2.4安装memcached2.4.1安装memcached1yum install -y memcached python-memcached 启动memcached12systemctl enable memcached.servicesystemctl start memcached.service","categories":[{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/categories/云计算/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"openstack","slug":"openstack","permalink":"http://www.duoyichen.xyz/tags/openstack/"},{"name":"云计算","slug":"云计算","permalink":"http://www.duoyichen.xyz/tags/云计算/"}]},{"title":"Lvs+Keepalived高可用负载均衡","slug":"lvs-kl","date":"2018-10-01T20:21:48.000Z","updated":"2018-10-02T14:13:01.068Z","comments":true,"path":"2018-10/lvs-kl/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/lvs-kl/","excerpt":"","text":"一. ARP协议1.1 什么是arp协议地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机，并接收返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。地址解析协议是建立在网络中各个主机互相信任的基础上的，网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时不会检测该报文的真实性就会将其记入本机ARP缓存；由此攻击者就可以向某一主机发送伪ARP应答报文，使其发送的信息无法到达预期的主机或到达错误的主机，这就构成了一个ARP欺骗。ARP命令可用于查询本机ARP缓存中IP地址和MAC地址的对应关系、添加或删除静态对应关系等。相关协议有RARP、代理ARP。NDP用于在IPv6中代替地址解析协议。 1.2 arp工作原理 主机A的IP地址为192.168.1.1，MAC地址为0A-11-22-33-44-01； 主机B的IP地址为192.168.1.2，MAC地址为0A-11-22-33-44-02； 当主机A要与主机B通信时，地址解析协议可以将主机B的IP地址（192.168.1.2）解析成主机B的MAC地址，以下为工作流程： 第1步：根据主机A上的路由表内容，IP确定用于访问主机B的转发IP地址是192.168.1.2。然后A主机在自己的本地ARP缓存中检查主机B的匹配MAC地址。 第2步：如果主机A在ARP缓存中没有找到映射，它将询问192.168.1.2的硬件地址，从而将ARP请求帧广播到本地网络上的所有主机。源主机A的IP地址和MAC地址都包括在ARP请求中。本地网络上的每台主机都接收到ARP请求并且检查是否与自己的IP地址匹配。如果主机发现请求的IP地址与自己的IP地址不匹配，它将丢弃ARP请求。 第3步：主机B确定ARP请求中的IP地址与自己的IP地址匹配，则将主机A的IP地址和MAC地址映射添加到本地ARP缓存中。 第4步：主机B将包含其MAC地址的ARP回复消息直接发送回主机A。 第5步：当主机A收到从主机B发来的ARP回复消息时，会用主机B的IP和MAC地址映射更新ARP缓存。本机缓存是有生存期的，生存期结束后，将再次重复上面的过程。主机B的MAC地址一旦确定，主机A就能向主机B发送IP通信了 1.3 arp代理C1和PC2虽然属于不同的广播域，但它们处于同一网段中，因此PC1会向PC2发出ARP请求广播包，请求获得PC2的MAC地址。由于路由器不会转发广播包，因此ARP请求只能到达路由器，不能到达PC2。当在路由器上启用ARP代理后，路由器会查看ARP请求，发现IP地址172.16.20.100属于它连接的另一个网络，因此路由器用自己的接口MAC地址代替PC2的MAC地址，向PC1发送了一个ARP应答。 二. LVS简介2.1 lvs概念LVS集群采用IP负载均衡技术和基于内容请求分发技术。调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序。为此，在设计时需要考虑系统的透明性、可伸缩性、高可用性和易管理性。 2.2 lvs三种基本模型2.2.1 lvs-DR模型是lvs的默认模型，也是企业中用到的最多的模型LVS_NAT模型，通常应用与rs较少，rs节点无要求，端口转换的场景 解读：地址转换模型，vs通过修改目的ip将报文发送到rs.rs通过dip网关将报文发给vs,vs再将报文的源ip进行修改发送给客户端 2.2.2 LVS-TUN 模式lvs-TUN模型可以运用于异地机房的负载调度上。 解读：隧道模型，跟DR模型比较相似，都是由rs直接回复给cs .跟dr模型不同的是，vs和rs之间可以存在路由，原因是tun模型在报文源ip和目的ip后又加入了一层源ip和目的ip的信息。 2.2.3 LVS-NAT 模式LVS_NAT模型，通常应用与rs较少，rs节点无要求，端口转换的场景 解读：地址转换模型，vs通过修改目的ip将报文发送到rs.rs通过dip网关将报文发给vs,vs再将报文的源ip进行修改发送给客户端。 三. LVS-DR模式详解 场景假设： 客户端：ip：192.168.1.1，mac：00：00：00：00：00：01 Lvs：vip：192.168.0.2，mac：00：00：00：00：00：02 Web服务器：ip：192.168.0.3，mac：00：00：00：00：00：03，lo网卡ip：192.168.0.2 Lvs工作流程流程： 客户向lvs发送一个请求后lvs收到的报文为源IP地址为：192.168.0.1，源mac为：00：00：00：00：00：01。目标IP地址为：192.168.0.2，目标mac地址为：00：00：00：00：00：02 当lvs收到请求后会根据自身算法选择一个web服务器对报文进行修改并转发给web服务器，此时转发的报文为IP地址为：192.168.0.1，源mac为：00：00：00：00：00：01，目标IP地址为：192.168.0.2，目标mac地址为：00：00：00：00：00：03 Web服务器收到lvs抓发过来的请求进行回复，此时web参看报文发现自己的lo网卡有目标地址那么进行恢复。回复的报文为源IP地址：192.168.0.2，源mac地址为：00：00：00：00：00：03，目标IP地址为：192.168.0.1，目标Imac、地址为：00：00：00：00：00：01 总结： 整个lvs-的dr模式客户端的计算机可以理解为arp欺骗，是客户的计算机始终认为和同一台计算机通信。 四. LVS的安装4.1 升级内核并安装内核开发包开启内核转发123456yum update -y kernelyum install -y kernel-develrebootln -s /usr/src/kernels/`uname -r` /usr/src/linuxsed -i 's#net.ipv4.ip_forward = 0#net.ipv4.ip_forward = 1#g' /etc/sysctl.conf sysctl -p 4.2 安装lvs1234567yum install popt-devel ipvsadm -yroot@template ~ 23:58:02 # lsmod | grep ip_vsroot@template ~ 23:58:14 # modprobe ip_vsroot@template ~ 23:58:23 # lsmod | grep ip_vsip_vs 126897 0 libcrc32c 1246 1 ip_vsipv6 336282 282 ip_vs,ip6t_REJECT,nf_conntrack_ipv6,nf_defrag_ipv6 五. LVS应用案例5.1 配置lvs主机ifconfig eth网卡号:编号 VIP netmask 255.255.255.0 up 12345678910111213[root@lvs01 ~]# ifconfig eth0:12 192.168.241.11 netmask 255.255.255.0 up[root@lvs01 ~]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 inet 192.168.241.11/24 brd 192.168.241.255 scope global secondary eth0:12 inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 5.2 添加转发VIPipvsadm -A -t VIP:port -s wrr -p 20 -s 参数使用wrr轮询算法 -p会话保持20秒 12[root@lvs01 ~]# ipvsadm -A -t 192.168.241.11:80 -s wrr -p 20[root@lvs01 ~]# 5.3 添加后端RSV（web）ipvsadm -a -t VIP:port -r RIP:port -g -w 1 123[root@lvs01 ~]# ipvsadm -a -t 192.168.241.11:80 -r 192.168.241.15:80 -g -w 1[root@lvs01 ~]# ipvsadm -a -t 192.168.241.11:80 -r 192.168.241.16:80 -g -w 1[root@lvs01 ~]# 5.4 查看设置12345678[root@lvs01 ~]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.241.11:80 wrr persistent 20 -&gt; 192.168.241.15:80 Route 1 0 0 -&gt; 192.168.241.16:80 Route 1 0 0 [root@lvs01 ~]# 5.5 配置后端rsv的lo网卡ifconfig lo:num VIP netmask 255.255.255.255 up 12345678910111213[root@apache ~]# ifconfig lo:0 192.168.241.11 netmask 255.255.255.255 up [root@apache ~]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet 192.168.241.11/32 brd 192.168.241.11 scope global lo:0 inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:d9:fc:b8 brd ff:ff:ff:ff:ff:ff inet 192.168.241.15/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fed9:fcb8/64 scope link valid_lft forever preferred_lft forever 5.6 抑制rsv的arp1234echo \"1\" &gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho \"1\" &gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho \"2\" &gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho \"2\" &gt; /proc/sys/net/ipv4/conf/all/arp_announce 5.7 用curl命令验证curl 192.168.241.11 六. KEEPALIVED介绍 Layer3,4&amp;7工作在IP/TCP协议栈的IP层，TCP层，及应用层,原理分别如下： Layer3：Keepalived使用Layer3的方式工作式时，Keepalived会定期向服务器群中的服务器发送一个ICMP的数据包（既我们平时用的Ping程序）,如果发现某台服务的IP地址没有激活，Keepalived便报告这台服务器失效，并将它从服务器群中剔除，这种情况的典型例子是某台服务器被非法关机。Layer3的方式是以服务器的IP地址是否有效作为服务器工作正常与否的标准。 Layer4:如果您理解了Layer3的方式，Layer4就容易了。Layer4主要以TCP端口的状态来决定服务器工作正常与否。如web server的服务端口一般是80，如果Keepalived检测到80端口没有启动，则Keepalived将把这台服务器从服务器群中剔除。 Layer7：Layer7就是工作在具体的应用层了，比Layer3,Layer4要复杂一点，在网络上占用的带宽也要大一些。Keepalived将根据用户的设定检查服务器程序的运行是否正常，如果与用户的设定不相符，则Keepalived将把服务器从服务器群中剔除。 七. 安装KEEPALIVED12345678910tar zxf keepalived-1.1.17.tar.gzcd keepalived-1.1.17./configuremake &amp;&amp; make installcp /usr/local/etc/rc.d/init.d/keepalived /etc/init.d cp /usr/local/etc/sysconfig/keepalived /etc/sysconfigcp /usr/local/etc/keepalived/keepalived.conf /etc/keepalived cp /usr/local/sbin/keepalive /usr/sbinchkconfig --add keepalived#注：使用lvs+keepalived高可用请在安装keepalived之前安装lvs软件，参见第四章 八. KEEPALIVED配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869global_defs &#123; notification_email &#123; znyang@vip.qq.com &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_1 #这行是keepalived服务器ID在同一个局域网内一定不能重复&#125;vrrp_script nginx_check &#123; #配置监控脚本 script &quot;/root/nginx_check.sh&quot; #监控脚本 interval 1 ###检测时间间隔 1s### weigh -60 ###如果条件成立（脚本返回非0），权重-60###&#125; vrrp_instance VI_1 &#123; state MASTER #MASTER 是主 BACKUP是从 interface eth0 #网络接口（需要绑定VIP的网卡） virtual_router_id 51 #配置实例的表示，在同一个配置文件中每个实例的标识是唯一的，被节点的要和主节点的一致 priority 100 #优先级数值越大优先级越高一般同一个实例中主比从大100 advert_int 1 #同步检查的时间间隔 track_script &#123; nginx_check #使用监控脚本 &#125; authentication &#123; auth_type PASS #权限类型是密码 auth_pass 1111 #密码 &#125; virtual_ipaddress &#123; 192.168.18.200 #虚拟路由器的地址（ ifconfig eth网卡号:编号 VIP netmask 255.255.255.0 up就是这个虚拟ip） &#125;&#125;######################若只做VIP切换则不需要下面的转发配置#####################################################virtual_server 192.168.18.200 8080 &#123; #配置虚拟主机 delay_loop 6 lb_algo wrr lb_kind DR nat_mask 255.255.255.0 persistence_timeout 50 protocol TCP real_server 192.168.18.203 8080 &#123; #第一个RS weight 1 TCP_CHECK &#123; connect_timeout 8 nb_get_retry 3 delay_before_retry 3 connect_port 8080 #健康检查的端口 &#125; &#125; real_server 192.168.18.204 8080 &#123; #第二个RS weight 1 TCP_CHECK &#123; connect_timeout 8 nb_get_retry 3 delay_before_retry 3 connect_port 8080 #健康检查的端口 &#125; &#125;&#125; 九. 生产实例9.1 只做高可用VIP切换9.1.1 按照上一章配置文件只复制到VIP切换部分到配置文件先清空配置文参照上文配置 9.1.2 分别启动主从keepalived1234[root@lvs01 keepalived]# /etc/init.d/keepalived startStarting keepalived: [ OK ][root@lvs02 keepalived]# /etc/init.d/keepalived startStarting keepalived: [ OK ] 9.1.3 查看主从IP1234567891011121314151617181920212223[root@lvs01 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 `inet 192.168.44.11/32 scope global eth0` inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever[root@lvs02 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever 9.1.4 停止主机keepalived模拟主机故障 主机信息 1234567891011[root@lvs01 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 从机信息 123456789101112[root@lvs02 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 `inet 192.168.44.11/32 scope global eth0` inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever 9.1.5 启动主机keepalived模拟故障恢复 主机信息 123456789101112[root@lvs01 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 `inet 192.168.44.11/32 scope global eth0` inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 从机信息 1234567891011[root@lvs02 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever 9.2 结合lvs做4成负载均衡9.2.1 修改主从两台主机的配置文件添加lvs部分参见上文 9.2.2 重启keepalived查看ipvs信息12345678910[root@lvs01 keepalived]# /etc/init.d/keepalived restartStopping keepalived: [ OK ]Starting keepalived: [ OK ][root@lvs01 keepalived]# ipvsadmIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.241.11:http wrr persistent 50 -&gt; 192.168.241.15:http Route 1 0 0 -&gt; 192.168.241.16:http Route 1 0 0 9.2.3 模拟主机故障 主机信息 123456[root@lvs01 keepalived]# /etc/init.d/keepalived stopStopping keepalived: [ OK ][root@lvs01 keepalived]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn 从机信息 12345678[root@lvs02 keepalived]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.241.11:80 wrr persistent 50 -&gt; 192.168.241.15:80 Route 1 0 0 -&gt; 192.168.241.16:80 Route 1 0 0 注：即使主故障，从接管，ipvs信息在从上并不会被删除，只会做VIP的切换 9.2.3 模拟故障恢复 主机信息 123456789101112[root@lvs01 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 inet 192.168.44.11/32 scope global eth0 inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 从机信息 1234567891011[root@lvs02 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever 十. 配置脚本触发主从切换10.1 添加监控脚本部分123456vrrp_script mysql_check &#123; #配置监控脚本 script &quot;/root/mysql_check.sh&quot; #监控脚本 interval 1 ###检测时间间隔 1s### weigh -100 ###如果条件成立（脚本返回非0），权重-100###&#125;#添加位置见第八章 10.2 给虚拟主机添加使用的脚本1234 track_script &#123; mysql_check #使用监控脚本 &#125;#添加位置见第八章 10.3 以mysql为例编写监控脚本123456#!/bin/bash mysqladmin ping &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then exit 0 fi exit 1 10.4 重启两个keepalived123456[root@lvs01 keepalived]# /etc/init.d/keepalived restartStopping keepalived: [ OK ]Starting keepalived: [ OK ][root@lvs02 keepalived]# /etc/init.d/keepalived restartStopping keepalived: [ OK ]Starting keepalived: [ OK ] 10.5 查看VIP 主机 123456789101112[root@lvs01 keepalived]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 inet 192.168.44.11/32 scope global eth0 inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 从机 1234567891011[root@lvs02 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever 10.6 停止主机mysql查看VIP 主机 12345678910111213[root@lvs01 keepalived]# /etc/init.d/mysqld stop Stopping mysqld: [ OK ][root@lvs01 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 从机 123456789101112[root@lvs02 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 `inet 192.168.44.11/32 scope global eth0` inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever 10.7 启动主机mysql恢复测试 主机 1234567891011121314[root@lvs01 keepalived]# /etc/init.d/mysqld startStarting mysqld: [ OK ][root@lvs01 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:cc:ec:94 brd ff:ff:ff:ff:ff:ff inet 192.168.241.12/24 brd 192.168.241.255 scope global eth0 `inet 192.168.44.11/32 scope global eth0` inet6 fe80::20c:29ff:fecc:ec94/64 scope link valid_lft forever preferred_lft forever 从机 1234567891011[root@lvs02 keepalived]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f4:f8:ab brd ff:ff:ff:ff:ff:ff inet 192.168.241.13/24 brd 192.168.241.255 scope global eth0 inet6 fe80::20c:29ff:fef4:f8ab/64 scope link valid_lft forever preferred_lft forever","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"keepalived","slug":"keepalived","permalink":"http://www.duoyichen.xyz/tags/keepalived/"},{"name":"lvs","slug":"lvs","permalink":"http://www.duoyichen.xyz/tags/lvs/"},{"name":"负载均衡","slug":"负载均衡","permalink":"http://www.duoyichen.xyz/tags/负载均衡/"}]},{"title":"Kubernets集群安装部署 06","slug":"k8s-6","date":"2018-10-01T20:19:57.000Z","updated":"2018-10-01T23:24:07.593Z","comments":true,"path":"2018-10/k8s-6/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/k8s-6/","excerpt":"","text":"七、部署Flannel7.1 为Flannel生成证书1234567891011121314151617181920cd /usr/local/src/sslcat &gt;flanneld-csr.json&lt;&lt;EOF&#123; &quot;CN&quot;: &quot;flanneld&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;TianJin&quot;, &quot;L&quot;: &quot;TianJin&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 7.2 生成证书1234cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld 7.3 分发证书123cp flanneld*.pem /opt/kubernetes/ssl/scp flanneld*.pem k8s-node1:/opt/kubernetes/ssl/scp flanneld*.pem k8s-node2:/opt/kubernetes/ssl/ 7.4安装Flannel12345678tar zxf flannel-v0.10.0-linux-amd64.tar.gzcp flanneld mk-docker-opts.sh /opt/kubernetes/bin/scp flanneld mk-docker-opts.sh k8s-node1:/opt/kubernetes/bin/scp flanneld mk-docker-opts.sh k8s-node2:/opt/kubernetes/bin/cd /usr/local/src/kubernetes/cluster/centos/node/bin/cp remove-docker0.sh /opt/kubernetes/bin/scp remove-docker0.sh k8s-node1:/opt/kubernetes/bin/scp remove-docker0.sh k8s-node2:/opt/kubernetes/bin/ 7.5 配置Flannel123456789cat &gt;/opt/kubernetes/cfg/flannel&lt;&lt;EOFFLANNEL_ETCD=&quot;-etcd-endpoints=https://192.168.217.10:2379,https://192.168.217.20:2379,https://192.168.217.30:2379&quot;FLANNEL_ETCD_KEY=&quot;-etcd-prefix=/kubernetes/network&quot;FLANNEL_ETCD_CAFILE=&quot;--etcd-cafile=/opt/kubernetes/ssl/ca.pem&quot;FLANNEL_ETCD_CERTFILE=&quot;--etcd-certfile=/opt/kubernetes/ssl/flanneld.pem&quot;FLANNEL_ETCD_KEYFILE=&quot;--etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem&quot;EOFscp /opt/kubernetes/cfg/flannel k8s-node1:/opt/kubernetes/cfg/scp /opt/kubernetes/cfg/flannel k8s-node2:/opt/kubernetes/cfg/ 7.6 设置Flannel系统服务1234567891011121314151617181920cat &gt;/usr/lib/systemd/system/flannel.service&lt;&lt;EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetBefore=docker.service[Service]EnvironmentFile=-/opt/kubernetes/cfg/flannelExecStartPre=/opt/kubernetes/bin/remove-docker0.shExecStart=/opt/kubernetes/bin/flanneld \\$&#123;FLANNEL_ETCD&#125; \\$&#123;FLANNEL_ETCD_KEY&#125; \\$&#123;FLANNEL_ETCD_CAFILE&#125; \\$&#123;FLANNEL_ETCD_CERTFILE&#125; \\$&#123;FLANNEL_ETCD_KEYFILE&#125;ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/dockerType=notify[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOFscp /usr/lib/systemd/system/flannel.service k8s-node1:/usr/lib/systemd/system/scp /usr/lib/systemd/system/flannel.service k8s-node2:/usr/lib/systemd/system/ 7.7 Flannel CNI集成12345cd /usr/local/src/mkdir /opt/kubernetes/bin/cnitar zxf cni-plugins-amd64-v0.7.1.tgz -C /opt/kubernetes/bin/cniscp -r /opt/kubernetes/bin/cni/* k8s-node1:/opt/kubernetes/bin/cni/scp -r /opt/kubernetes/bin/cni/* k8s-node2:/opt/kubernetes/bin/cni/ 7.8 创建Etcd的key123/opt/kubernetes/bin/etcdctl --ca-file /opt/kubernetes/ssl/ca.pem --cert-file /opt/kubernetes/ssl/flanneld.pem --key-file /opt/kubernetes/ssl/flanneld-key.pem \\ --no-sync -C https://192.168.217.10:2379,https://192.168.217.20:2379,https://192.168.217.30:2379 \\mk /kubernetes/network/config &apos;&#123; &quot;Network&quot;: &quot;10.2.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot;, &quot;VNI&quot;: 1 &#125;&#125;&apos; &gt;/dev/null 2&gt;&amp;1 7.9 修改docker启动文件(再所有节点执行)12345678910[root@k8s-master ~]# vim /usr/lib/systemd/system/docker.service[Unit] #在Unit下面修改After和增加RequiresAfter=network-online.target firewalld.service flannel.serviceWants=network-online.targetRequires=flannel.service[Service] #增加EnvironmentFile=-/run/flannel/dockerType=notifyEnvironmentFile=-/run/flannel/dockerExecStart=/usr/bin/dockerd $DOCKER_OPTS 7.10 分发docker启动文件12scp /usr/lib/systemd/system/docker.service k8s-node1:/usr/lib/systemd/system/scp /usr/lib/systemd/system/docker.service k8s-node2:/usr/lib/systemd/system/ 7.11重启docker(再所有节点执行)12systemctl daemon-reloadsystemctl restart docker","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://www.duoyichen.xyz/tags/kubernets/"},{"name":"docker","slug":"docker","permalink":"http://www.duoyichen.xyz/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://www.duoyichen.xyz/tags/容器/"}]},{"title":"Kubernets集群安装部署 05","slug":"k8s-5","date":"2018-10-01T20:18:57.000Z","updated":"2018-10-01T23:23:52.324Z","comments":true,"path":"2018-10/k8s-5/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/k8s-5/","excerpt":"","text":"六、部署node节点6.1 安装软件(在master执行)1234tar zxf kubernetes-node-linux-amd64.tar.gz.tar cd /usr/local/src/kubernetes/server/bin/scp kubelet kube-proxy k8s-node1:/opt/kubernetes/bin/scp kubelet kube-proxy k8s-node2:/opt/kubernetes/bin/ 6.2 创建角色(在master执行)1kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap 6.3 创建 kubelet bootstrapping kubeconfig 文件 设置集群参数(在master执行)12345kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.217.10:6443 \\ --kubeconfig=bootstrap.kubeconfig 6.4 设置客户端认证参数(在master执行)123kubectl config set-credentials kubelet-bootstrap \\ --token=$token\\ --kubeconfig=bootstrap.kubeconfig 6.5 设置上下文参数(在master执行)1234kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig 6.6 默认选择上下文(在master执行)123kubectl config use-context default --kubeconfig=bootstrap.kubeconfigscp bootstrap.kubeconfig k8s-node1:/opt/kubernetes/cfgscp bootstrap.kubeconfig k8s-node2:/opt/kubernetes/cfg 6.7 设置CNI支持(在所有node执行)123456789101112mkdir -p /etc/cni/net.dcat &gt;/etc/cni/net.d/10-default.conf&lt;&lt;EOF&#123; &quot;name&quot;: &quot;flannel&quot;, &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: &#123; &quot;bridge&quot;: &quot;docker0&quot;, &quot;isDefaultGateway&quot;: true, &quot;mtu&quot;: 1400 &#125;&#125;EOF 6.8 创建kubelet目录(在所有node执行)1mkdir /var/lib/kubelet 6.9 创建kubelet服务配置node112345678910111213141516171819202122232425262728293031cat &gt;/usr/lib/systemd/system/kubelet.service&lt;&lt;EOF[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/opt/kubernetes/bin/kubelet \\ --address=192.168.217.20 \\ --hostname-override=192.168.217.20 \\ --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\ --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\ --cert-dir=/opt/kubernetes/ssl \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/opt/kubernetes/bin/cni \\ --cluster-dns=10.1.0.2 \\ --cluster-domain=cluster.local. \\ --hairpin-mode hairpin-veth \\ --allow-privileged=true \\ --fail-swap-on=false \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5EOF node212345678910111213141516171819202122232425262728293031323334cat &gt;/usr/lib/systemd/system/kubelet.service&lt;&lt;EOF[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/opt/kubernetes/bin/kubelet \\ --address=192.168.217.30 \\ --hostname-override=192.168.217.30 \\ --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\ --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\ --cert-dir=/opt/kubernetes/ssl \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/opt/kubernetes/bin/cni \\ --cluster-dns=10.1.0.2 \\ --cluster-domain=cluster.local. \\ --hairpin-mode hairpin-veth \\ --allow-privileged=true \\ --fail-swap-on=false \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF 6.10 启动kubelet(在所有node执行)123systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubelet 6.11 验证kubelet(在所有node执行)12345678910111213141516171819202122[root@k8s-node2 ~]# systemctl status kubelet● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; static; vendor preset: disabled) Active: active (running) since 四 2018-06-07 13:34:07 CST; 32s ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 25392 (kubelet) Tasks: 6 Memory: 12.6M CGroup: /system.slice/kubelet.service └─25392 /opt/kubernetes/bin/kubelet --address=192.168.217.30 --hostname-override=192.168.217.30 --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg...6月 07 13:34:07 k8s-node2 systemd[1]: kubelet.service holdoff time over, scheduling restart.6月 07 13:34:07 k8s-node2 systemd[1]: Started Kubernetes Kubelet.6月 07 13:34:07 k8s-node2 systemd[1]: Starting Kubernetes Kubelet...6月 07 13:34:08 k8s-node2 kubelet[25392]: Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet&apos;s --config flag. See https://kubernetes.io/docs/tasks/admini...ore information.6月 07 13:34:08 k8s-node2 kubelet[25392]: Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet&apos;s --config flag. See https://kubernetes.io/docs/tasks/ad...ore information.6月 07 13:34:08 k8s-node2 kubelet[25392]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet&apos;s --config flag. See https://kubernetes.io/docs/tasks...ore information.6月 07 13:34:08 k8s-node2 kubelet[25392]: Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet&apos;s --config flag. See https://kubernetes.io/docs/tasks/a...ore information.6月 07 13:34:08 k8s-node2 kubelet[25392]: Flag --allow-privileged has been deprecated, will be removed in a future version6月 07 13:34:08 k8s-node2 kubelet[25392]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet&apos;s --config flag. See https://kubernetes.io/docs/tasks/a...ore information.Hint: Some lines were ellipsized, use -l to show in full.[root@k8s-node2 ~]# 6.12 查看csr请求(在master执行)1234[root@k8s-master /usr/local/src/kubernetes/server/bin 13:34:07]# kubectl get csr NAME AGE REQUESTOR CONDITIONnode-csr-VKSZrqXu0FOwgkcrWKgb2LsTngcCJfV8xJ4GjLC65r4 48s kubelet-bootstrap Pendingnode-csr-nzkrupg8ngY7em9qSpSJ5QRJCcwcdk0alyNDuRnyCNE 52s kubelet-bootstrap Pending 6.13 批准kubelet 的 TLS 证书请求(在master执行)1kubectl get csr|grep &apos;Pending&apos; | awk &apos;NR&gt;0&#123;print $1&#125;&apos;| xargs kubectl certificate approve 6.14 安装lvs(在所有node执行)1yum install -y ipvsadm ipset conntrack 6.15 创建 kube-proxy 证书请求(在master执行)1234567891011121314151617181920cd /usr/local/src/ssl/cat &gt;kube-proxy-csr.json&lt;&lt;EOF&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;TianJin&quot;, &quot;L&quot;: &quot;TianJin&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 6.16 生成kube-proxy证书(在master执行)1234567cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxycp kube-proxy*.pem /opt/kubernetes/ssl/scp kube-proxy*.pem k8s-node1:/opt/kubernetes/ssl/scp kube-proxy*.pem k8s-node2:/opt/kubernetes/ssl/ 6.17 创建kube-proxy配置文件(在master执行)123456789101112131415161718kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.217.10:6443 \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\ --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfigcp kube-proxy.kubeconfig /opt/kubernetes/cfg/scp kube-proxy.kubeconfig k8s-node1:/opt/kubernetes/cfg/scp kube-proxy.kubeconfig k8s-node2:/opt/kubernetes/cfg/ 6.18 创建kube-proxy服务配置node112345678910111213141516171819202122232425262728293031mkdir /var/lib/kube-proxycat &gt;/usr/lib/systemd/system/kube-proxy.service&lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \\ --bind-address=192.168.217.20 \\ --hostname-override=192.168.217.20 \\ --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \\--masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF node212345678910111213141516171819202122232425262728293031mkdir /var/lib/kube-proxycat &gt;/usr/lib/systemd/system/kube-proxy.service&lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \\ --bind-address=192.168.217.30 \\ --hostname-override=192.168.217.30 \\ --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \\--masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 6.19 启动kube-proxy(在所有node执行)123systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxy 6.20 验证(在master执行)1234[root@k8s-master /usr/local/src/ssl 13:46:34]# kubectl get nodeNAME STATUS ROLES AGE VERSION192.168.217.20 Ready &lt;none&gt; 12m v1.10.3192.168.217.30 Ready &lt;none&gt; 12m v1.10.3","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://www.duoyichen.xyz/tags/kubernets/"},{"name":"docker","slug":"docker","permalink":"http://www.duoyichen.xyz/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://www.duoyichen.xyz/tags/容器/"}]},{"title":"Kubernets集群安装部署 04","slug":"k8s-4","date":"2018-10-01T20:16:57.000Z","updated":"2018-10-01T23:24:05.179Z","comments":true,"path":"2018-10/k8s-4/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/k8s-4/","excerpt":"","text":"5.7 启动API Server服务123systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserver 5.8 验证API Server123456789101112131415161718192021[root@k8s-master /usr/local/src/kubernetes 11:19:42]# systemctl status kube-apiserver ● kube-apiserver.service - Kubernetes API Server Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled) Active: active (running) since 四 2018-06-07 11:19:42 CST; 8s ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 22837 (kube-apiserver) Tasks: 7 Memory: 303.5M CGroup: /system.slice/kube-apiserver.service └─22837 /opt/kubernetes/bin/kube-apiserver --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction --bind-address=192.168.217.10 --insecure-bind-address=127.0...6月 07 11:19:35 k8s-master systemd[1]: kube-apiserver.service holdoff time over, scheduling restart.6月 07 11:19:35 k8s-master systemd[1]: Starting Kubernetes API Server...6月 07 11:19:36 k8s-master kube-apiserver[22837]: Flag --admission-control has been deprecated, Use --enable-admission-plugins or --disable-admission-plugins instead. Will be removed in a future version.6月 07 11:19:36 k8s-master kube-apiserver[22837]: Flag --insecure-bind-address has been deprecated, This flag will be removed in a future version.6月 07 11:19:38 k8s-master kube-apiserver[22837]: [restful] 2018/06/07 11:19:38 log.go:33: [restful/swagger] listing is available at https://192.168.217.10:6443/swaggerapi6月 07 11:19:38 k8s-master kube-apiserver[22837]: [restful] 2018/06/07 11:19:38 log.go:33: [restful/swagger] https://192.168.217.10:6443/swaggerui/ is mapped to folder /swagger-ui/6月 07 11:19:39 k8s-master kube-apiserver[22837]: [restful] 2018/06/07 11:19:39 log.go:33: [restful/swagger] listing is available at https://192.168.217.10:6443/swaggerapi6月 07 11:19:39 k8s-master kube-apiserver[22837]: [restful] 2018/06/07 11:19:39 log.go:33: [restful/swagger] https://192.168.217.10:6443/swaggerui/ is mapped to folder /swagger-ui/6月 07 11:19:42 k8s-master systemd[1]: Started Kubernetes API Server.[root@k8s-master /usr/local/src/kubernetes 11:19:50]# 5.9 部署Controller Manager服务12345678910111213141516171819202122232425262728cat &gt;/usr/lib/systemd/system/kube-controller-manager.service&lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/opt/kubernetes/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.1.0.0/16 \\ --cluster-cidr=10.2.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/opt/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF 5.10 启动Controller Manager服务123systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-manager 5.11 验证Controller Manager服务1234567891011121314[root@k8s-master /usr/local/src/kubernetes 11:45:24]# systemctl status kube-controller-manager● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since 四 2018-06-07 11:45:24 CST; 8s ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 22894 (kube-controller) Tasks: 7 Memory: 10.1M CGroup: /system.slice/kube-controller-manager.service └─22894 /opt/kubernetes/bin/kube-controller-manager --address=127.0.0.1 --master=http://127.0.0.1:8080 --allocate-node-cidrs=true --service-cluster-ip-range=10.1.0.0/16 --cluster-cidr=10.2.0.0/16 --cluster-name=kuberne...6月 07 11:45:24 k8s-master systemd[1]: Started Kubernetes Controller Manager.6月 07 11:45:24 k8s-master systemd[1]: Starting Kubernetes Controller Manager...6月 07 11:45:26 k8s-master kube-controller-manager[22894]: E0607 11:45:26.105012 22894 core.go:75] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail 5.12 部署Kubernetes Scheduler1234567891011121314151617181920cat &gt;/usr/lib/systemd/system/kube-scheduler.service&lt;&lt;EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/opt/kubernetes/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF 5.13 启动Kubernetes Scheduler123systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-scheduler 5.14 验证Kubernetes Scheduler1234567891011121314[root@k8s-master /usr/local/src/kubernetes 11:48:35]# systemctl status kube-scheduler● kube-scheduler.service - Kubernetes Scheduler Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since 四 2018-06-07 11:48:35 CST; 7s ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 22942 (kube-scheduler) Tasks: 6 Memory: 8.2M CGroup: /system.slice/kube-scheduler.service └─22942 /opt/kubernetes/bin/kube-scheduler --address=127.0.0.1 --master=http://127.0.0.1:8080 --leader-elect=true --v=2 --logtostderr=false --log-dir=/opt/kubernetes/log6月 07 11:48:35 k8s-master systemd[1]: Started Kubernetes Scheduler.6月 07 11:48:35 k8s-master systemd[1]: Starting Kubernetes Scheduler...[root@k8s-master /usr/local/src/kubernetes 11:48:42]# 5.15 部署kubectl命令123tar zxf kubernetes-client-linux-amd64.tar.gz.tar cd /usr/local/src/kubernetes/client/bincp kubectl /opt/kubernetes/bin/ 5.16 创建 admin 证书签名请求1234567891011121314151617181920cd /usr/local/src/ssl/cat &gt; admin-csr.json&lt;&lt;EOF&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;TianJin&quot;, &quot;L&quot;: &quot;TianJin&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 5.17 生成admin证书、私钥12345cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare adminmv admin*.pem /opt/kubernetes/ssl/ 5.18 设置集群参数1234kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.217.10:6443 5.19 设置客户端认证参数1234kubectl config set-credentials admin \\ --client-certificate=/opt/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/opt/kubernetes/ssl/admin-key.pem 5.20 设置上下文参数123kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin 5.21 设置默认上下文1kubectl config use-context kubernetes 5.22 使用kubectl命令验证12345678[root@k8s-master /usr/local/src/ssl 11:58:48]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; controller-manager Healthy ok etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; [root@k8s-master /usr/local/src/ssl 11:58:58]#","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://www.duoyichen.xyz/tags/kubernets/"},{"name":"docker","slug":"docker","permalink":"http://www.duoyichen.xyz/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://www.duoyichen.xyz/tags/容器/"}]},{"title":"Kubernets集群安装部署 03","slug":"k8s-3","date":"2018-10-01T20:15:57.000Z","updated":"2018-10-01T23:24:01.653Z","comments":true,"path":"2018-10/k8s-3/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/k8s-3/","excerpt":"","text":"五、安装Master5.1 安装软件12345tar zxf kubernetes-server-linux-amd64.tar.gz.tarcd kubernetescp server/bin/kube-apiserver /opt/kubernetes/bin/cp server/bin/kube-controller-manager /opt/kubernetes/bin/cp server/bin/kube-scheduler /opt/kubernetes/bin/ 5.2 创建生成CSR的 JSON 配置文件12345678910111213141516171819202122232425262728cat &gt;&gt;kubernetes-csr.json&lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.217.10&quot;, &quot;10.1.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;TianJin&quot;, &quot;L&quot;: &quot;TianJin&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 5.3 生成 kubernetes 证书和私钥1234567cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetescp kubernetes*.pem /opt/kubernetes/ssl/scp kubernetes*.pem k8s-node1:/opt/kubernetes/ssl/scp kubernetes*.pem k8s-node2:/opt/kubernetes/ssl/ 5.4 创建 kube-apiserver 使用的客户端 token 文件12token=$(head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;) echo &quot;$token,kubelet-bootstrap,10001,\\&quot;system:kubelet-bootstrap\\&quot;&quot; &gt; /opt/kubernetes/ssl/bootstrap-token.csv 5.5 创建基础用户名/密码认证配置1234cat &gt;/opt/kubernetes/ssl/basic-auth.csv&lt;&lt;EOFadmin,admin,1readonly,readonly,2EOF 5.6 创建API Server服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546cat &gt;/usr/lib/systemd/system/kube-apiserver.service&lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]ExecStart=/opt/kubernetes/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --bind-address=192.168.217.10 \\ --insecure-bind-address=127.0.0.1 \\ --authorization-mode=Node,RBAC \\ --runtime-config=rbac.authorization.k8s.io/v1 \\ --kubelet-https=true \\ --anonymous-auth=false \\ --basic-auth-file=/opt/kubernetes/ssl/basic-auth.csv \\ --enable-bootstrap-token-auth \\ --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv \\ --service-cluster-ip-range=10.1.0.0/16 \\ --service-node-port-range=20000-40000 \\ --tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\ --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/opt/kubernetes/ssl/ca.pem \\ --etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=https://192.168.217.10:2379,https://192.168.217.20:2379,https://192.168.217.30:2379 \\ --enable-swagger-ui=true \\ --allow-privileged=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/opt/kubernetes/log/api-audit.log \\ --event-ttl=1h \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/logRestart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://www.duoyichen.xyz/tags/kubernets/"},{"name":"docker","slug":"docker","permalink":"http://www.duoyichen.xyz/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://www.duoyichen.xyz/tags/容器/"}]},{"title":"Kubernets集群安装部署 02","slug":"k8s-2","date":"2018-10-01T20:14:57.000Z","updated":"2018-10-01T23:23:59.041Z","comments":true,"path":"2018-10/k8s-2/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/k8s-2/","excerpt":"","text":"四、部署ETCD集群4.1安装ETCD(在master执行)12345tar zxf etcd-v3.3.6-linux-amd64.tar.gz cd etcd-v3.3.6-linux-amd64cp etcd etcdctl /opt/kubernetes/bin/ scp etcd etcdctl k8s-node1:/opt/kubernetes/bin/scp etcd etcdctl k8s-node2:/opt/kubernetes/bin/ 4.2 创建etcd证书签名请求(在master执行)123456789101112131415161718192021222324cat &gt;&gt;etcd-csr.json&lt;&lt;EOF&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.217.10&quot;, &quot;192.168.217.20&quot;, &quot;192.168.217.30&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;TianJin&quot;, &quot;L&quot;: &quot;TianJin&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 4.3 生成 etcd 证书和私钥(在master执行)123456789101112131415161718[root@k8s-master /usr/local/src/ssl 09:40:24]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd2018/06/07 09:41:02 [INFO] generate received request2018/06/07 09:41:02 [INFO] received CSR2018/06/07 09:41:02 [INFO] generating key: rsa-20482018/06/07 09:41:02 [INFO] encoded CSR2018/06/07 09:41:02 [INFO] signed certificate with serial number 5729987940323194957822218358988843687619354814242018/06/07 09:41:02 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@k8s-master /usr/local/src/ssl 09:41:02]# ls -l etcd*-rw-r--r--. 1 root root 1086 6月 7 09:41 etcd.csr-rw-r--r--. 1 root root 288 6月 7 09:38 etcd-csr.json-rw-------. 1 root root 1675 6月 7 09:41 etcd-key.pem-rw-r--r--. 1 root root 1456 6月 7 09:41 etcd.pem 4.4 将证书移动到/opt/kubernetes/ssl目录下(在master执行)1234cp etcd*.pem /opt/kubernetes/sslscp etcd*.pem k8s-node1:/opt/kubernetes/sslscp etcd*.pem k8s-node2:/opt/kubernetes/sslrm -f etcd.csr etcd-csr.json 4.5 设置ETCD配置文件master配置123456789101112131415161718192021222324252627282930cat &gt;&gt;/opt/kubernetes/cfg/etcd.conf&lt;&lt;EOF#[member]ETCD_NAME=&quot;etcd-node1&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#ETCD_SNAPSHOT_COUNTER=&quot;10000&quot;#ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;#ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.217.10:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.217.10:2379,https://127.0.0.1:2379&quot;#ETCD_MAX_SNAPSHOTS=&quot;5&quot;#ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.217.10:2380&quot;# if you use different ETCD_NAME (e.g. test),# set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-node1=https://192.168.217.10:2380,etcd-node2=https://192.168.217.20:2380,etcd-node3=https://192.168.217.30:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;k8s-etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.217.10:2379&quot;#[security]CLIENT_CERT_AUTH=&quot;true&quot;ETCD_CA_FILE=&quot;/opt/kubernetes/ssl/ca.pem&quot;ETCD_CERT_FILE=&quot;/opt/kubernetes/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/opt/kubernetes/ssl/etcd-key.pem&quot;PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CA_FILE=&quot;/opt/kubernetes/ssl/ca.pem&quot;ETCD_PEER_CERT_FILE=&quot;/opt/kubernetes/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/opt/kubernetes/ssl/etcd-key.pem&quot;EOF node1而配置123456789101112131415161718192021222324252627282930cat &gt;&gt;/opt/kubernetes/cfg/etcd.conf&lt;&lt;EOF#[member]ETCD_NAME=&quot;etcd-node2&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#ETCD_SNAPSHOT_COUNTER=&quot;10000&quot;#ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;#ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.217.20:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.217.20:2379,https://127.0.0.1:2379&quot;#ETCD_MAX_SNAPSHOTS=&quot;5&quot;#ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.217.20:2380&quot;# if you use different ETCD_NAME (e.g. test),# set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-node1=https://192.168.217.10:2380,etcd-node2=https://192.168.217.20:2380,etcd-node3=https://192.168.217.30:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;k8s-etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.217.20:2379&quot;#[security]CLIENT_CERT_AUTH=&quot;true&quot;ETCD_CA_FILE=&quot;/opt/kubernetes/ssl/ca.pem&quot;ETCD_CERT_FILE=&quot;/opt/kubernetes/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/opt/kubernetes/ssl/etcd-key.pem&quot;PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CA_FILE=&quot;/opt/kubernetes/ssl/ca.pem&quot;ETCD_PEER_CERT_FILE=&quot;/opt/kubernetes/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/opt/kubernetes/ssl/etcd-key.pem&quot;EOF node2配置123456789101112131415161718192021222324252627282930cat &gt;&gt;/opt/kubernetes/cfg/etcd.conf&lt;&lt;EOF#[member]ETCD_NAME=&quot;etcd-node3&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#ETCD_SNAPSHOT_COUNTER=&quot;10000&quot;#ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;#ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.217.30:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.217.30:2379,https://127.0.0.1:2379&quot;#ETCD_MAX_SNAPSHOTS=&quot;5&quot;#ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.217.30:2380&quot;# if you use different ETCD_NAME (e.g. test),# set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-node1=https://192.168.217.10:2380,etcd-node2=https://192.168.217.20:2380,etcd-node3=https://192.168.217.30:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;k8s-etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https:/192.168.217.30:2379&quot;#[security]CLIENT_CERT_AUTH=&quot;true&quot;ETCD_CA_FILE=&quot;/opt/kubernetes/ssl/ca.pem&quot;ETCD_CERT_FILE=&quot;/opt/kubernetes/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/opt/kubernetes/ssl/etcd-key.pem&quot;PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CA_FILE=&quot;/opt/kubernetes/ssl/ca.pem&quot;ETCD_PEER_CERT_FILE=&quot;/opt/kubernetes/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/opt/kubernetes/ssl/etcd-key.pem&quot;EOF 4.4 创建etcd服务(在所有节点执行)12345678910111213141516cat &gt;&gt;/etc/systemd/system/etcd.service&lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.target[Service]Type=simpleWorkingDirectory=/var/lib/etcdEnvironmentFile=-/opt/kubernetes/cfg/etcd.conf# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /opt/kubernetes/bin/etcd&quot;Type=notify[Install]WantedBy=multi-user.targetEOF 4.5 启动ectd集群(在所有节点执行)1234systemctl daemon-reloadsystemctl enable etcdmkdir /var/lib/etcdsystemctl start etcd 4.6 检查集群状态(在master执行)123456789[root@k8s-master /usr/local/src/ssl 10:56:14]# etcdctl --endpoints=https://192.168.217.10:2379 \\ --ca-file=/opt/kubernetes/ssl/ca.pem \\ --cert-file=/opt/kubernetes/ssl/etcd.pem \\ --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-healthmember 3c012a87c4e193ac is healthy: got healthy result from https://192.168.217.20:2379member 3cb279d5dff870db is healthy: got healthy result from https://192.168.217.30:2379member 92b780d24fec7c16 is healthy: got healthy result from https://192.168.217.10:2379cluster is healthy[root@k8s-master /usr/local/src/ssl 10:56:26]#","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://www.duoyichen.xyz/tags/kubernets/"},{"name":"docker","slug":"docker","permalink":"http://www.duoyichen.xyz/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://www.duoyichen.xyz/tags/容器/"}]},{"title":"Kubernets集群安装部署 01","slug":"k8s","date":"2018-10-01T20:13:57.000Z","updated":"2018-10-01T23:23:56.868Z","comments":true,"path":"2018-10/k8s/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/k8s/","excerpt":"","text":"一、环境约定 主机名 IP 安装的软件 kube-master 192.168.217.10 etcd、kube-apiserver、kube-controller、kube-scheduler、kubectl kube-node1 192.168.217.20 etcd、kube-node、kube-proxy kube-node2 192.168.217.30 etcd、kube-node、kube-proxy 二、环境准备2.1 创建目录添加PATH(在所有节点执行)123mkdir -p /opt/kubernetes/&#123;cfg,bin,ssl,log&#125;echo &quot;export PATH=/opt/kubernetes/bin:\\$PATH&quot; &gt;&gt; /root/.bash_profile source /root/.bash_profile 2.2 安装docker(在所有节点执行)123curl -sSL https://get.daocloud.io/docker | shsystemctl start docker.service systemctl enable docker.service 2.3 修改host(在所有节点执行)12345cat &gt;&gt;/etc/hosts/&lt;&lt;EOF192.168.217.10 k8s-master192.168.217.20 k8s-node1192.168.217.30 k8s-node2EOF 2.4拉取测试镜像(可不执行)12docker pull daocloud.io/library/nginx:1.13.2docker pull daocloud.io/library/nginx:1.10.2 2.5 配置免密码登录(在master执行)123ssh-keygen -t rsassh-copy-id k8s-node1ssh-copy-id k8s-node2 三、制作CA证书(在master节点操作)3.1 将程序拷贝到相应目录并授权执行1234chmod +x cfssl*mv cfssl-certinfo_linux-amd64 /opt/kubernetes/bin/cfssl-certinfomv cfssljson_linux-amd64 /opt/kubernetes/bin/cfssljsonmv cfssl_linux-amd64 /opt/kubernetes/bin/cfssl 3.2 拷贝文件到其他节点12scp /opt/kubernetes/bin/cfssl* k8s-node1:/opt/kubernetes/binscp /opt/kubernetes/bin/cfssl* k8s-node2:/opt/kubernetes/bin 3.3 初始化123mkdir ssl &amp;&amp; cd sslcfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.json 3.4 创建用来生成 CA 文件的 JSON 配置文件1234567891011121314151617181920cat &gt;&gt;ca-config.json&lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;8760h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;8760h&quot; &#125; &#125; &#125;&#125;EOF 3.5 创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件123456789101112131415161718cat &gt;&gt;ca-csr.json&lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;TianJin&quot;, &quot;L&quot;: &quot;TianJin&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 3.6生成CA证书（ca.pem）和密钥（ca-key.pem）12345678910111213[root@k8s-master ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca2018/06/07 09:30:16 [INFO] generating a new CA key and certificate from CSR2018/06/07 09:30:16 [INFO] generate received request2018/06/07 09:30:16 [INFO] received CSR2018/06/07 09:30:16 [INFO] generating key: rsa-20482018/06/07 09:30:17 [INFO] encoded CSR2018/06/07 09:30:17 [INFO] signed certificate with serial number 442587652045208440377444170631446076181330771637[root@k8s-master ssl]# ls -l ca*-rw-r--r--. 1 root root 290 6月 7 09:11 ca-config.json-rw-r--r--. 1 root root 1001 6月 7 09:30 ca.csr-rw-r--r--. 1 root root 208 6月 7 09:12 ca-csr.json-rw-------. 1 root root 1675 6月 7 09:30 ca-key.pem-rw-r--r--. 1 root root 1359 6月 7 09:30 ca.pem 3.7 颁发证书123cp ca.csr ca.pem ca-key.pem ca-config.json /opt/kubernetes/sslscp ca.csr ca.pem ca-key.pem ca-config.json k8s-node1:/opt/kubernetes/ssl scp ca.csr ca.pem ca-key.pem ca-config.json k8s-node2:/opt/kubernetes/ssl","categories":[{"name":"运维","slug":"运维","permalink":"http://www.duoyichen.xyz/categories/运维/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://www.duoyichen.xyz/tags/kubernets/"},{"name":"docker","slug":"docker","permalink":"http://www.duoyichen.xyz/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://www.duoyichen.xyz/tags/容器/"}]},{"title":"CMDB资产管理系统","slug":"cmdb","date":"2018-10-01T20:02:22.000Z","updated":"2018-10-01T23:25:51.534Z","comments":true,"path":"2018-10/cmdb/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/cmdb/","excerpt":"","text":"随着互联网的进步与发展，企业的IT环境越来越复杂，数量庞大、品类繁多。如何有效地管理IT设备，降低成本，并为业提供高效的IT服务，成为很多企业面临的问题。CMDB正是来解决这些问题的，同时为运维自动化，监控报警等提供支持。 开发环境：Windows10 + Python3.5 + PyCharm5 项目架构：Python3.5 + sqlite3 + Django2.1 + AdminLTE 1. 登录界面： 2. Django后台管理： 3. 资产审批： 4. 仪表盘： 5. 资产总表： 6. 资产详情： 7. 代码架构：","categories":[{"name":"开发","slug":"开发","permalink":"http://www.duoyichen.xyz/categories/开发/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.duoyichen.xyz/tags/python/"},{"name":"cmdb","slug":"cmdb","permalink":"http://www.duoyichen.xyz/tags/cmdb/"}]},{"title":"Hexo静态博客搭建指南","slug":"firsr-blog","date":"2018-10-01T01:27:47.000Z","updated":"2018-10-01T20:48:11.597Z","comments":true,"path":"2018-10/firsr-blog/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/firsr-blog/","excerpt":"","text":"GitHub Pages 本用于介绍托管在 GitHub 的项目，也可以用来搭建博客，有300M免费空间。 hexo是一个基于Node.js的静态博客程序，可以方便的生成静态网页托管在github和Heroku上。作者是来自台湾的 tommy351 。 优势： 生成静态页面快 支持 Markdown 兼容于 Windows, Mac &amp; Linux 部署方便。日常使用仅需五个命令。 高扩展性、自订性，文件少、小，易理解 1. 安装 &amp; 搭建 安装 Git：安装后，注册 Github 账号，配置 SSH（具体见下一步）,打开 Git Bash,接下来的命令均在Git Bash中执行 安装 Node.js 安装 Hexo : $ npm install -g hexo 安装依赖包： $ npm install 新建博客文件夹：cd到该文件夹，执行$hexo init 新建Github仓库：仓库名必须为你的 Github名.github.io，要不然就不能使用Github Pages服务了。。。 2. 配置 SSH 关于什么是 SSH，请自行百度（我懒）这里直接讲一下配置步骤。 本地生成公钥私钥 $ ssh-keygen -t rsa -C &quot;你的邮件地址&quot; 添加公钥到 Github 根据上一步的提示，找到公钥文件（默认为id_rsa.pub），用记事本打开，全选并复制。 登录 Github，右上角 头像 -&gt; Settings —&gt; SSH keys —&gt; Add SSH key。把公钥粘贴到key中，填好title并点击 Add key。 git bash中输入命令 $ ssh -T git@github.com ，选yes，等待片刻可看到成功提示。 3. NexT主题下载 NexT 主题是由 iissnan 大神所制作的一款简洁美观不失逼格的主题。下载方法有以下两种： 进入博客根目录/themes/, 执行$ git clone https://github.com/iissnan/hexo-theme-next.git 直接进入上面的链接，在项目主页download zip文件，然后解压到博客根目录/themes/ 文件夹 4. 发布 使用以下两条命令进行发布，发布成功后可在浏览器中使用你的github名.github.io进入你的博客~ 12$ hexo clean$ hexo d -g","categories":[{"name":"其他","slug":"其他","permalink":"http://www.duoyichen.xyz/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://www.duoyichen.xyz/tags/其他/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-10-01T01:07:47.000Z","updated":"2018-10-01T19:06:13.090Z","comments":true,"path":"2018-10/hello-world/","link":"","permalink":"http://www.duoyichen.xyz/2018-10/hello-world/","excerpt":"","text":"前几天，开发了一套 CMDB系统 ，上传到github 。 为了展示效果，开发的时候也涉及到前端的一些东西。最近，打算搞一下 python 运维开发。开发项目，前端也很重要。正好趁十一这几天，在家研究一下前端设计以及网站制作方面的东西。 Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"其他","slug":"其他","permalink":"http://www.duoyichen.xyz/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://www.duoyichen.xyz/tags/其他/"}]}]}